{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, confusion_matrix, accuracy_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om-clbkuirNJ"
      },
      "outputs": [],
      "source": [
        "conllu_dir = 'data/output_conllu/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_stopwords(file_path):\n",
        "    stopwords = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            word = line.strip()\n",
        "            stopwords.append(word)\n",
        "    return stopwords\n",
        "\n",
        "lista_stopwords = load_stopwords('data/stopwords-it.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lista_stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_files = []\n",
        "test_files = []\n",
        "\n",
        "for file_name in os.listdir(conllu_dir):\n",
        "    print(file_name)\n",
        "    if 'train' in file_name:\n",
        "        train_files.append(file_name)\n",
        "    elif 'test' in file_name:\n",
        "        test_files.append(file_name)\n",
        "\n",
        "print('Documenti training set:', len(train_files))\n",
        "print('Documenti test set:', len(test_files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_files[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_sentences_from_file(src_path):\n",
        "    user_sentences = []                \n",
        "    sentence = []                      \n",
        "    for line in open(src_path, 'r', encoding = \"utf-8\"):\n",
        "        if line[0].isdigit():          \n",
        "            splitted_line = line.strip().split('\\t')\n",
        "            if '-' not in splitted_line[0] and splitted_line[1].lower() not in lista_stopwords:  ì\n",
        "                token = {\n",
        "                    'word': splitted_line[1],\n",
        "                    'lemma': splitted_line[2],\n",
        "                    'pos': splitted_line[3]\n",
        "                }\n",
        "                sentence.append(token)\n",
        "        if line == '\\n':  ì\n",
        "            user_sentences.append(sentence)\n",
        "            sentence = []\n",
        "    return user_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = []\n",
        "\n",
        "for user_path in train_files:\n",
        "    user_sentences = get_sentences_from_file('data/output_conllu/' + user_path)\n",
        "    train_dataset.append(user_sentences)\n",
        "\n",
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#estrae gli n-grammi di parole dalla frase data\n",
        "def extract_word_ngrams(word_ngrams, sentence, el, n):\n",
        "    \n",
        "    all_words = []\n",
        "    for token in sentence:\n",
        "        all_words.append(token[el])\n",
        "    \n",
        "    for i in range(0, len(all_words) - n + 1):\n",
        "        ngram_words = all_words[i: i + n]\n",
        "        ngram = f'{el.upper()}_{n}_' + '_'.join(ngram_words)\n",
        "        if ngram not in word_ngrams:\n",
        "            word_ngrams[ngram] = 1 #aggiorna il dizionario con l'n-gramma e la sua frequenza (se non esiste già)\n",
        "        else:\n",
        "            word_ngrams[ngram] += 1 #incrementa la frequenza dell'n-gramma nel dizionario\n",
        "    \n",
        "    return word_ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#estrae gli n-grammi di caratteri dalla frase data\n",
        "def extract_char_ngrams(char_ngrams, sentence, n):\n",
        "    \n",
        "    all_words = []\n",
        "    for token in sentence:\n",
        "        all_words.append(token['word'])\n",
        "\n",
        "    all_words = ' '.join(all_words)\n",
        "    \n",
        "    for i in range(0, len(all_words) - n + 1):\n",
        "        ngram_chars = all_words[i:i + n]\n",
        "        ngram = f'CHAR_{n}_' + ngram_chars\n",
        "\n",
        "        if ngram not in char_ngrams:\n",
        "            char_ngrams[ngram] = 1\n",
        "        else:\n",
        "            char_ngrams[ngram] += 1\n",
        "    \n",
        "    return char_ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "#conta il numero totale di parole nel documento.\n",
        "\n",
        "def count_document_words(document):\n",
        "    num_words = 0\n",
        "    for sentence in document:\n",
        "        num_words = num_words + len(sentence)\n",
        "    return num_words\n",
        "\n",
        "#conta il numero totale di caratteri nel documento.\n",
        "\n",
        "def count_document_chars(document):\n",
        "    num_chars = 0\n",
        "    for sentence in document:\n",
        "        for token in sentence:\n",
        "            num_chars = num_chars + len(token['word'])\n",
        "        num_chars = num_chars + len(sentence) - 1  \n",
        "    return num_chars\n",
        "\n",
        "#normalizza i conteggi degli n-grammi nel dizionario ngrams_dict dividendo per la lunghezza totale del documento\n",
        "\n",
        "def normalize_ngrams(ngrams_dict, doc_len):\n",
        "    for ngram in ngrams_dict:\n",
        "        ngrams_dict[ngram] = ngrams_dict[ngram]/float(doc_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for document in train_dataset:\n",
        "    for frase in document:\n",
        "        print(frase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#estrazione delle features basate su n-grammi \n",
        "def extract_features(dataset):\n",
        "    dataset_features = []\n",
        "    for document in dataset:\n",
        "        word_unigrams = dict()\n",
        "        word_bigrams = dict()\n",
        "        word_trigrams = dict()\n",
        "        #lemmas_unigrams = dict()\n",
        "        #pos_unigrams = dict()\n",
        "        #pos_bigrams= dict()\n",
        "        #pos_trigrams = dict()\n",
        "        #lemmas_bigrams = dict()\n",
        "        char_trigrams = dict()\n",
        "        char_bigrams = dict()\n",
        "        #lemmas_trigrams = dict()\n",
        "        for sentence in document:\n",
        "            #extract_word_ngrams(pos_unigrams, sentence, 'pos', 1)\n",
        "            #extract_word_ngrams(pos_bigrams, sentence, 'pos', 2)\n",
        "            #extract_word_ngrams(pos_trigrams, sentence, 'pos', 3)\n",
        "            extract_char_ngrams(char_trigrams, sentence, 3)\n",
        "            extract_char_ngrams(char_bigrams, sentence, 2)\n",
        "            #extract_word_ngrams(lemmas_bigrams, sentence, 'lemma', 2)\n",
        "            #extract_word_ngrams(lemmas_trigrams, sentence, 'lemma', 3)\n",
        "            #extract_word_ngrams(lemmas_unigrams, sentence, 'lemma', 1)\n",
        "            #extract_char_ngrams(char_fourgrams, sentence, 4)\n",
        "            extract_word_ngrams(word_unigrams, sentence, 'word', 1)\n",
        "            extract_word_ngrams(word_bigrams, sentence, 'word', 2)\n",
        "            extract_word_ngrams(word_trigrams, sentence, 'word', 3)\n",
        "\n",
        "        num_words = count_document_words(document)\n",
        "        num_chars = count_document_chars(document)\n",
        "        #normalize_ngrams(pos_unigrams, num_words)\n",
        "        #normalize_ngrams(pos_bigrams, num_words)\n",
        "        #normalize_ngrams(pos_trigrams, num_words)\n",
        "        normalize_ngrams(char_trigrams, num_chars)\n",
        "        normalize_ngrams(char_bigrams, num_chars)\n",
        "        #normalize_ngrams(lemmas_bigrams, num_words)\n",
        "        #normalize_ngrams(lemmas_trigrams, num_words)\n",
        "        #normalize_ngrams(lemmas_unigrams, num_words)\n",
        "        #normalize_ngrams(char_fourgrams, num_chars)\n",
        "        normalize_ngrams(word_unigrams, num_words)\n",
        "        normalize_ngrams(word_bigrams, num_words)\n",
        "        normalize_ngrams(word_trigrams, num_words)\n",
        "        \n",
        "        user_features = char_bigrams | char_trigrams | word_unigrams | word_bigrams | word_trigrams\n",
        "      \n",
        "        dataset_features.append(user_features)\n",
        "    return dataset_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_features = extract_features(train_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_features[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_num_features(dataset):\n",
        "    all_features = set()\n",
        "    for user_dict in dataset:\n",
        "        all_features.update(list(user_dict.keys()))\n",
        "    return len(all_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'Numero features: {get_num_features(train_features)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#filtra le feature nel training set rispetto a un'occorrenza minima \n",
        "def filter_features(train_features, min_occurrences):\n",
        "    \n",
        "    features_counter = dict()\n",
        "    for user_dict in train_features:\n",
        "        for feature in user_dict:\n",
        "            if feature in features_counter:\n",
        "                features_counter[feature] += 1\n",
        "            else:\n",
        "                features_counter[feature] = 1\n",
        "\n",
        "    for user_dict in train_features:\n",
        "        user_features = list(user_dict.keys())\n",
        "        for feature in user_features:\n",
        "            if features_counter[feature] < min_occurrences:\n",
        "                user_dict.pop(feature)\n",
        "                \n",
        "    return train_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_features = extract_features(train_dataset)\n",
        "train_features = filter_features(train_features, 5)\n",
        "print(f'Numero features dopo il filtro: {get_num_features(train_features)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "vectorizer = DictVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "X_train = scaler.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_label_train(dataset):\n",
        "    labels_irony = []\n",
        "    labels_sarcasm = []\n",
        "    for user_list in dataset:\n",
        "    \n",
        "        #file_name = user_list[0]\n",
        "        file_name = user_list[:-len('.conllu')]\n",
        "        splitted_file_name = file_name.split('_')\n",
        "\n",
        "        irony = splitted_file_name[2]\n",
        "        sarcasm = splitted_file_name[3]\n",
        "\n",
        "        labels_irony.append(irony)\n",
        "        labels_sarcasm.append(sarcasm)\n",
        "    return labels_irony, labels_sarcasm\n",
        "\n",
        "def create_label_test(dataset):\n",
        "    labels_irony = []\n",
        "    labels_sarcasm = []\n",
        "    for user_list in dataset:\n",
        "\n",
        "        file_name = user_list[:-len('.conllu')]\n",
        "        splitted_file_name = file_name.split('_')\n",
        "\n",
        "        irony = splitted_file_name[2]\n",
        "        sarcasm = splitted_file_name[3]\n",
        "\n",
        "        labels_irony.append(irony)\n",
        "        labels_sarcasm.append(sarcasm)\n",
        "    return labels_irony, labels_sarcasm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_labels_irony, train_labels_sarcasm = create_label_train(train_files)\n",
        "test_labels_irony, test_labels_sarcasm = create_label_test(test_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_labels_irony"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svc = LinearSVC(dual=True, max_iter=10000)\n",
        "svc.fit(X_train, train_labels_irony)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "train_predictions = svc.predict(X_train)\n",
        "print(classification_report(train_labels_irony, train_predictions, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train = np.asarray(train_labels_irony)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "splitter = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "folds = list(splitter.split(X_train))\n",
        "\n",
        "for i in range(len(folds)):\n",
        "    print(len(folds[i][0]), len(folds[i][1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "fold_accuracies = []\n",
        "\n",
        "for i in range(len(folds)):\n",
        "    train_ids = folds[i][0]\n",
        "    test_ids = folds[i][1]\n",
        "\n",
        "    fold_X_train = X_train[train_ids]\n",
        "    fold_y_train = y_train[train_ids]\n",
        "\n",
        "    fold_X_test = X_train[test_ids]\n",
        "    fold_y_test = y_train[test_ids]\n",
        "\n",
        "    kfold_svc = LinearSVC(dual=False)\n",
        "    kfold_svc.fit(fold_X_train, fold_y_train)\n",
        "    fold_y_pred = kfold_svc.predict(fold_X_test)\n",
        "    fold_accuracy = accuracy_score(fold_y_test, fold_y_pred)\n",
        "    fold_accuracies.append(fold_accuracy)\n",
        "\n",
        "    dummy_clf = DummyClassifier(strategy=\"most_frequent\")   # Dummy classifier viene utilizzato per avere una baseline\n",
        "    dummy_clf.fit(fold_X_train, fold_y_train)\n",
        "    dummy_score = dummy_clf.score(fold_X_test, fold_y_test)\n",
        "\n",
        "    all_y_true += fold_y_test.tolist()\n",
        "    all_y_pred += fold_y_pred.tolist()\n",
        "    print(f\"Accuracy fold {i+1}: {fold_accuracy}, baseline: {dummy_score}\")\n",
        "\n",
        "# Calcola l'accuracy media sui 5 fold\n",
        "mean_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
        "print(f\"Accuracy media sui 5 fold: {mean_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataset = []\n",
        "\n",
        "for file_path in test_files:\n",
        "    doc_sentences = get_sentences_from_file('data/output_conllu/' + file_path)\n",
        "    test_dataset.append(doc_sentences)\n",
        "\n",
        "test_labels_irony, test_labels_sarcasm = create_label_test(test_files)\n",
        "test_features = extract_features(test_dataset)\n",
        "\n",
        "X_test = vectorizer.transform(test_features)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_labels_irony"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_predictions = svc.predict(X_test)\n",
        "print(classification_report(test_labels_irony, test_predictions,  zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "coefs = svc.coef_ \n",
        "coefs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_names = vectorizer.get_feature_names_out(X_train).tolist()\n",
        "features_names;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "idx = 0\n",
        "class_coefs = coefs[idx]\n",
        "\n",
        "feature_importances = {feature_name: coef for feature_name, coef in zip(features_names, class_coefs)}\n",
        "sorted_feature_importances = dict(sorted(feature_importances.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "\n",
        "num_to_plot = 15\n",
        "print(f'Feature importance classe {svc.classes_[idx]}')\n",
        "plt.barh(range(num_to_plot), list(sorted_feature_importances.values())[:num_to_plot], align='center')\n",
        "plt.yticks(range(num_to_plot), list(sorted_feature_importances.keys())[:num_to_plot])\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
