{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68624706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7909e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lettura(corpus):\n",
    "    file_lettura = open(corpus, \"r\", encoding = \"utf-8\")\n",
    "    file_lettura = file_lettura.read()\n",
    "    return file_lettura[:100000]\n",
    "\n",
    "def lunghezzeCorpus(file):\n",
    "    frasi = sent_tokenize(file) #suddivido il testo in frasi utilizzando la funzione sent_tokenize del modulo nltk\n",
    "    tokens = [] \n",
    "    for frase in frasi:\n",
    "        tokens += word_tokenize(frase) #suddivido ogni frase in parole, che aggiungo alla lista 'tokens'\n",
    "    #conto il numero di frasi e di tokens\n",
    "    numFrasi = len(frasi)\n",
    "    numTokens = len(tokens)\n",
    "    return numFrasi, numTokens, tokens\n",
    "\n",
    "def lunghezzeMedie(file, numTokens):\n",
    "    frasi = sent_tokenize(file)\n",
    "    lungMediaFrasi = numTokens/len(frasi)\n",
    "    num_caratteri = 0\n",
    "    tokens = []\n",
    "    punteggiatura = [\",\", \"''\", \".\", \":\", \"?\", \"!\", \"\\\"\", \"/\", \";\", \"-\"]\n",
    "    for frase in frasi:\n",
    "        tokens += word_tokenize(frase)\n",
    "    for token in tokens:\n",
    "        if token not in punteggiatura:\n",
    "            num_caratteri += len(token)\n",
    "    lungMediaCaratteri = num_caratteri / len(tokens)\n",
    "    return lungMediaFrasi, lungMediaCaratteri\n",
    "\n",
    "def numeroHapax(tokens):\n",
    "    incremento = [500, 1000, 3000, len(tokens)-1]\n",
    "    hapax_500 = 0\n",
    "    hapax_1000 = 0\n",
    "    hapax_3000 = 0\n",
    "    hapax_corpus = 0\n",
    "    for i in range(len(incremento)):\n",
    "        testo = tokens[0:incremento[i]]\n",
    "        for token in testo:\n",
    "            tokens_hapax = testo.count(token)\n",
    "            if incremento[i] == 500 and tokens_hapax == 1:\n",
    "                hapax_500 += 1\n",
    "            if incremento[i] == 1000 and tokens_hapax == 1:\n",
    "                hapax_1000 += 1\n",
    "            if incremento[i] == 3000 and tokens_hapax == 1:\n",
    "                hapax_3000 += 1\n",
    "            if incremento[i] == len(tokens)-1 and tokens_hapax == 1:\n",
    "                hapax_corpus += 1\n",
    "    return hapax_500, hapax_1000, hapax_3000, hapax_corpus\n",
    "\n",
    "def ttr_token(tokens):\n",
    "    for i in range(0, len(tokens), 1000):\n",
    "        frammento_testo = tokens[0:i+1000]\n",
    "        dizionario = FreqDist(frammento_testo)\n",
    "        ttr = len(dizionario) / len(frammento_testo)\n",
    "        print(f'Vocabolario dopo {i+1000} tokens: {len(dizionario)}')\n",
    "        print(f'TTR: {ttr}')\n",
    "\n",
    "def lemmatizzatore(file):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(file)\n",
    "    lemmi = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "    numero_lemmi = len(set(lemmi))\n",
    "    return numero_lemmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "283c283f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il primo file, delitto_castigo.txt, contiene 22053 tokens e 828 frasi.\n",
      "\n",
      "Il secondo file, analisi_mente.txt, contiene 19783 tokens e 684 frasi.\n",
      "\n",
      "La lunghezza media delle frasi di delitto_castigo.txt è di 26.634 tokens, mentre la lunghezza media di ciascun token è di 3.578 caratteri\n",
      "\n",
      "La lunghezza media delle frasi di analisi_mente.txt è di 28.923 tokens, mentre la lunghezza media di ciascun token è di 4.065 caratteri\n",
      "\n",
      "delitto_castigo.txt contiene 187 hapax dopo 500 token,\n",
      "312 hapax dopo 1000 token,\n",
      "580 hapax dopo 3000 token\n",
      "e 1684 hapax nell'intero corpus\n",
      "\n",
      "analisi_mente.txt contiene 159 hapax dopo 500 token,\n",
      "272 hapax dopo 1000 token,\n",
      "476 hapax dopo 3000 token\n",
      "e 1520 hapax nell'intero corpus\n",
      "\n",
      "Vocabolario dopo 1000 tokens: 427\n",
      "TTR: 0.427\n",
      "Vocabolario dopo 2000 tokens: 705\n",
      "TTR: 0.3525\n",
      "Vocabolario dopo 3000 tokens: 894\n",
      "TTR: 0.298\n",
      "Vocabolario dopo 4000 tokens: 1083\n",
      "TTR: 0.27075\n",
      "Vocabolario dopo 5000 tokens: 1297\n",
      "TTR: 0.2594\n",
      "Vocabolario dopo 6000 tokens: 1464\n",
      "TTR: 0.244\n",
      "Vocabolario dopo 7000 tokens: 1609\n",
      "TTR: 0.22985714285714287\n",
      "Vocabolario dopo 8000 tokens: 1743\n",
      "TTR: 0.217875\n",
      "Vocabolario dopo 9000 tokens: 1853\n",
      "TTR: 0.2058888888888889\n",
      "Vocabolario dopo 10000 tokens: 1957\n",
      "TTR: 0.1957\n",
      "Vocabolario dopo 11000 tokens: 2048\n",
      "TTR: 0.18618181818181817\n",
      "Vocabolario dopo 12000 tokens: 2167\n",
      "TTR: 0.18058333333333335\n",
      "Vocabolario dopo 13000 tokens: 2287\n",
      "TTR: 0.17592307692307693\n",
      "Vocabolario dopo 14000 tokens: 2391\n",
      "TTR: 0.1707857142857143\n",
      "Vocabolario dopo 15000 tokens: 2484\n",
      "TTR: 0.1656\n",
      "Vocabolario dopo 16000 tokens: 2599\n",
      "TTR: 0.1624375\n",
      "Vocabolario dopo 17000 tokens: 2710\n",
      "TTR: 0.15941176470588236\n",
      "Vocabolario dopo 18000 tokens: 2788\n",
      "TTR: 0.15488888888888888\n",
      "Vocabolario dopo 19000 tokens: 2857\n",
      "TTR: 0.15036842105263157\n",
      "Vocabolario dopo 20000 tokens: 2954\n",
      "TTR: 0.1477\n",
      "Vocabolario dopo 21000 tokens: 3026\n",
      "TTR: 0.14409523809523808\n",
      "Vocabolario dopo 22000 tokens: 3102\n",
      "TTR: 0.141\n",
      "Vocabolario dopo 23000 tokens: 3110\n",
      "TTR: 0.1410238969754682\n",
      "\n",
      "Vocabolario dopo 1000 tokens: 388\n",
      "TTR: 0.388\n",
      "Vocabolario dopo 2000 tokens: 608\n",
      "TTR: 0.304\n",
      "Vocabolario dopo 3000 tokens: 787\n",
      "TTR: 0.2623333333333333\n",
      "Vocabolario dopo 4000 tokens: 917\n",
      "TTR: 0.22925\n",
      "Vocabolario dopo 5000 tokens: 1095\n",
      "TTR: 0.219\n",
      "Vocabolario dopo 6000 tokens: 1259\n",
      "TTR: 0.20983333333333334\n",
      "Vocabolario dopo 7000 tokens: 1422\n",
      "TTR: 0.20314285714285715\n",
      "Vocabolario dopo 8000 tokens: 1559\n",
      "TTR: 0.194875\n",
      "Vocabolario dopo 9000 tokens: 1711\n",
      "TTR: 0.19011111111111112\n",
      "Vocabolario dopo 10000 tokens: 1837\n",
      "TTR: 0.1837\n",
      "Vocabolario dopo 11000 tokens: 1983\n",
      "TTR: 0.18027272727272728\n",
      "Vocabolario dopo 12000 tokens: 2078\n",
      "TTR: 0.17316666666666666\n",
      "Vocabolario dopo 13000 tokens: 2223\n",
      "TTR: 0.171\n",
      "Vocabolario dopo 14000 tokens: 2327\n",
      "TTR: 0.1662142857142857\n",
      "Vocabolario dopo 15000 tokens: 2445\n",
      "TTR: 0.163\n",
      "Vocabolario dopo 16000 tokens: 2536\n",
      "TTR: 0.1585\n",
      "Vocabolario dopo 17000 tokens: 2680\n",
      "TTR: 0.15764705882352942\n",
      "Vocabolario dopo 18000 tokens: 2763\n",
      "TTR: 0.1535\n",
      "Vocabolario dopo 19000 tokens: 2831\n",
      "TTR: 0.149\n",
      "Vocabolario dopo 20000 tokens: 2884\n",
      "TTR: 0.14578173178992063\n",
      "\n",
      "Il primo file, delitto_castigo.txt, ha in totale 2789 lemmi\n",
      "Il secondo file, analisi_mente.txt, ha in totale 2458 lemmi\n"
     ]
    }
   ],
   "source": [
    "file1 = \"delitto_castigo.txt\"\n",
    "file2 = \"analisi_mente.txt\"\n",
    "\n",
    "def main(corpus1, corpus2):\n",
    "    lettura_file1 = lettura(corpus1)\n",
    "    lettura_file2 = lettura(corpus2)\n",
    "    \n",
    "    #CONFRONTO I DUE CORPORA RISPETTO A TOKEN E NUMERO DI FRASI \n",
    "    \n",
    "    num_frasi1, num_tokens1, tokens1 = lunghezzeCorpus(lettura_file1)\n",
    "    num_frasi2, num_tokens2, tokens2 = lunghezzeCorpus(lettura_file2)\n",
    "    \n",
    "    print(f'Il primo file, {corpus1}, contiene {num_tokens1} tokens e {num_frasi1} frasi.')\n",
    "    print()\n",
    "    print(f'Il secondo file, {corpus2}, contiene {num_tokens2} tokens e {num_frasi2} frasi.')\n",
    "    \n",
    "    #CONFRONTO LA LUNGHEZZA MEDIA DELLE FRASI IN TOKEN E LA LUNGHEZZA MEDIA DEI TOKEN IN CARATTERI, PUNTEGGIATURA ESCLUSA\n",
    "    \n",
    "    lungMedia_frasi1, lungMedia_caratteri1 = lunghezzeMedie(lettura_file1, num_tokens1)\n",
    "    lungMedia_frasi2, lungMedia_caratteri2 = lunghezzeMedie(lettura_file2, num_tokens2)\n",
    "    print()\n",
    "    \n",
    "    print(f'La lunghezza media delle frasi di {corpus1} è di {lungMedia_frasi1:.3f} tokens, mentre la lunghezza media di ciascun token è di {lungMedia_caratteri1:.3f} caratteri')\n",
    "    print()\n",
    "    print(f'La lunghezza media delle frasi di {corpus2} è di {lungMedia_frasi2:.3f} tokens, mentre la lunghezza media di ciascun token è di {lungMedia_caratteri2:.3f} caratteri')\n",
    "    \n",
    "    #CONFRONTO IL NUMERO DI HAPAX DOPO 500, 1000, 3000 TOKENS E, INFINE, NELL'INTERO CORPUS\n",
    "    \n",
    "    print()\n",
    "    hapax_500_1, hapax_1000_1, hapax_3000_1, hapax_corpus_1 = numeroHapax(tokens1)\n",
    "    hapax_500_2, hapax_1000_2, hapax_3000_2, hapax_corpus_2 = numeroHapax(tokens2)\n",
    "    \n",
    "    print(f'{corpus1} contiene {hapax_500_1} hapax dopo 500 token,\\n{hapax_1000_1} hapax dopo 1000 token,\\n{hapax_3000_1} hapax dopo 3000 token\\ne {hapax_corpus_1} hapax nell\\'intero corpus')\n",
    "    print(f'\\n{corpus2} contiene {hapax_500_2} hapax dopo 500 token,\\n{hapax_1000_2} hapax dopo 1000 token,\\n{hapax_3000_2} hapax dopo 3000 token\\ne {hapax_corpus_2} hapax nell\\'intero corpus')\n",
    "    \n",
    "    #ANALIZZO LA CRESCITA DEL VOCABOLARIO E LA RICCHEZZA LESSICALE (TTR) OGNI 1000 TOKEN PER ENTRAMBI I CORPORA\n",
    "    print()\n",
    "    ttr_token(tokens1)\n",
    "    print()\n",
    "    ttr_token(tokens2)\n",
    "    \n",
    "    #CONTO IL NUMERO DI LEMMI DISTINTI PER OGNI CORPORA\n",
    "    print()\n",
    "    numero_lemmi1 = lemmatizzatore(lettura_file1)\n",
    "    numero_lemmi2 = lemmatizzatore(lettura_file2)\n",
    "    print(f'Il primo file, {file1}, ha in totale {numero_lemmi1} lemmi')\n",
    "    print(f'Il secondo file, {file2}, ha in totale {numero_lemmi2} lemmi')\n",
    "    \n",
    "main(file1, file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b38ac04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
