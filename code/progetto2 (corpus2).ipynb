{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "234d5e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import *\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf90a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lettura(corpus):\n",
    "    with open(corpus, \"r\", encoding = \"utf-8\") as infile:\n",
    "        file_lettura = infile.read()\n",
    "        return file_lettura[:100000]\n",
    "\n",
    "def analisiCorpus(file):\n",
    "    frasi = sent_tokenize(file)\n",
    "    tokens = []\n",
    "    for frase in frasi:\n",
    "        tokens += word_tokenize(frase)\n",
    "    pos_tagging = pos_tag(tokens)\n",
    "    return frasi, tokens, pos_tagging\n",
    "\n",
    "def pos_ngrammi(tokens_tag, n):\n",
    "    pos_tag = [tag for token, tag in tokens_tag] #creo una lista di pos tag dai token associati \n",
    "    pos_ngrammi = list(ngrams(pos_tag, n)) #creo una lista di n-grammi di pos utilizzando la funzione ngrams\n",
    "    frequenza_ngrammi = FreqDist(pos_ngrammi) #utilizzo il modulo FreqDist per calcola la frequenza degli n-grammi\n",
    "    #ottengo i 10 n-grammi più frequenti di pos tag utilizzando il metodo most_common\n",
    "    top_ngrammi = frequenza_ngrammi.most_common(10) \n",
    "    return top_ngrammi\n",
    "\n",
    "def top_tags(tokens, pos):\n",
    "    #creo una lista di token filtrando quelli che hanno il pos tag richiesto\n",
    "    filtra_token = [token for token, pos_tag in tokens if pos_tag == pos]\n",
    "    freq_token = Counter(filtra_token) #utilizzo la funzione Counter per contare le occorrenze di ogni token nella lista filtrata\n",
    "    top_tokens = freq_token.most_common(20) #ottengo i 20 tokens più frequenti\n",
    "    return top_tokens\n",
    "\n",
    "def bigrammi_nomi_aggettivi(tokens_tag):\n",
    "    #uso la funzione bigrams del modulo nltk per creare una lista di bigrammi di pos \n",
    "    bigrammi = list(bigrams([pos for token, pos in tokens_tag]))\n",
    "    aggettivo_nome = [] #lista vuota che conterrà tutti i bigrammi aggettivo-nome\n",
    "    for i in range(len(tokens_tag)-1):\n",
    "        #seleziono tutti i bigrammi che contengono un aggettivo seguito da un nome\n",
    "        if bigrammi[i][0] == \"JJ\" and bigrammi [i][1] == \"NN\":\n",
    "            aggettivo_nome.append((tokens_tag[i][0], tokens_tag[i+1][0])) #aggiungo il bigramma alla lista\n",
    "    freq_dist = FreqDist(aggettivo_nome)\n",
    "    top_bigrammi = freq_dist.most_common(20)\n",
    "    return top_bigrammi, aggettivo_nome\n",
    "\n",
    "    \n",
    "def prob_cond(tokens, bigrammi):\n",
    "    frequenze_unigrammi = FreqDist(tokens)\n",
    "    probabilità_condizionate = {} #dizionario vuoto che conterrà le probabilità condizionate\n",
    "    conteggio_token = FreqDist(bigrammi)\n",
    "    for x, y in conteggio_token.items():\n",
    "        if frequenze_unigrammi[x[0]] > 1: #filtro solo parole non hapax al denominatore\n",
    "            denominatore = frequenze_unigrammi[x[0]] #corrisponde alla sequenza del token che condiziona il bigramma\n",
    "            numeratore = y #corrisponde alla frequenza del bigramma\n",
    "            probabilità = numeratore/denominatore\n",
    "            probabilità_condizionate[x] = probabilità #aggiungo la probabilità condizionata al dizionario\n",
    "    #ordino il dizionario di probabilità in maniera decrescente\n",
    "    top_prob = sorted(probabilità_condizionate.items(), key = lambda x: x[1], reverse = True)[:20]\n",
    "    return top_prob\n",
    "\n",
    "def mutual_information(tokens, bigrammi):\n",
    "    PMI = {} #inizializzo il dizionario che conterrà la PMI per ogni bigramma \n",
    "    conteggio_token = FreqDist(tokens)\n",
    "    conteggio_bigrammi = FreqDist(bigrammi)\n",
    "    for x, y in conteggio_bigrammi.items():\n",
    "        numeratore = y * len(tokens) #moltiplico la frequenza del bigramma per la lunghezza del corpus\n",
    "        #moltiplico la probabilità del primo termine del bigramma per quella del secondo\n",
    "        denominatore = (conteggio_token[x[0]]/len(tokens)) * (conteggio_token[x[1]]/len(tokens))\n",
    "        calcolo_PMI = numeratore/denominatore\n",
    "        PMI[x] = math.log2(calcolo_PMI) #applico il logaritmo in base 2 alla mia probabilità\n",
    "    top_pmi = sorted(PMI.items(), key = lambda x: x[1], reverse = True)[:20]\n",
    "    return top_pmi\n",
    "\n",
    "def distribuzione_frasi_max(tokens, frasi):\n",
    "    freq_token = FreqDist(tokens)\n",
    "    frasi_filtrate = []\n",
    "    #calcolo la frequenza dei token e seleziono quelli non hapax\n",
    "    non_hapax = [token for token, freq in freq_token.items() if freq >= 2]\n",
    "    #filtro le frasi in base alla loro lunghezza e alla presenza di token non hapax\n",
    "    for frase in frasi:\n",
    "        tokens_frase = word_tokenize(frase)\n",
    "        if len(tokens_frase) >= 10 and len(tokens_frase) <= 20:\n",
    "            frequenza_token = 0\n",
    "            for token in tokens_frase:\n",
    "                if token in non_hapax:\n",
    "                    frequenza_token += 1\n",
    "            if frequenza_token >= len(tokens_frase) // 2:\n",
    "                frasi_filtrate.append(tokens_frase)\n",
    "    #seleziono la frase con la valutazione media di frequenza dei token maggiore tra le frasi filtrate\n",
    "    max_freq = 0\n",
    "    frase_freq_max = \"\"\n",
    "    for sentence in frasi_filtrate:\n",
    "        somma_frequenza = 0\n",
    "        for token in sentence:\n",
    "            somma_frequenza += freq_token[token]\n",
    "        freq_media = somma_frequenza / len(sentence)\n",
    "        if freq_media > max_freq:\n",
    "            max_freq = freq_media\n",
    "            frase_freq_max = sentence\n",
    "    #restituisco la frase selezionata (sotto forma di stringa) \n",
    "    return \" \".join(frase_freq_max)\n",
    "\n",
    "def distribuzione_frasi_min(tokens, frasi):\n",
    "    freq_token = FreqDist(tokens)\n",
    "    frasi_filtrate = []\n",
    "    non_hapax = [token for token, freq in freq_token.items() if freq >= 2]\n",
    "    for frase in frasi:\n",
    "        tokens_frase = word_tokenize(frase)\n",
    "        if len(tokens_frase) >= 10 and len(tokens_frase) <= 20:\n",
    "            frequenza_token = 0\n",
    "            for token in tokens_frase:\n",
    "                if token in non_hapax:\n",
    "                    frequenza_token += 1\n",
    "            if frequenza_token >= len(tokens_frase) // 2:\n",
    "                frasi_filtrate.append(tokens_frase)\n",
    "    #seleziono la fraso con la valutazione media di frequenza dei token minore tra le frasi filtrate\n",
    "    min_freq = 1000\n",
    "    frase_freq_min = \"\"\n",
    "    for sentence in frasi_filtrate:\n",
    "        somma_frequenza = 0\n",
    "        for token in sentence:\n",
    "            somma_frequenza += freq_token[token]\n",
    "        freq_media = somma_frequenza / len(sentence)\n",
    "        if freq_media < min_freq:\n",
    "            min_freq = freq_media\n",
    "            frase_freq_min = sentence\n",
    "    return \" \".join(frase_freq_min)\n",
    "\n",
    "def markov2(tokens, frasi):\n",
    "    freq_token = FreqDist(tokens)\n",
    "    frasi_filtrate = []\n",
    "    non_hapax = [token for token, freq in freq_token.items() if freq >= 2]\n",
    "    for frase in frasi:\n",
    "        tokens_frase = word_tokenize(frase)\n",
    "        if len(tokens_frase) >= 10 and len(tokens_frase) <= 20:\n",
    "            frequenza_token = 0\n",
    "            for token in tokens_frase:\n",
    "                if token in non_hapax:\n",
    "                    frequenza_token += 1\n",
    "            if frequenza_token >= len(tokens_frase) // 2:\n",
    "                frasi_filtrate.append(tokens_frase)\n",
    "    #calcolo la frase tra quelle filtrate con la probabilità massima tramite la catena di Markov di ordine 2\n",
    "    frase_max = \"\"\n",
    "    prob_max = 0\n",
    "    bigrammi_corpus = list(bigrams(tokens))\n",
    "    trigrammi_corpus = list(trigrams(tokens))\n",
    "    for sentence in frasi_filtrate:\n",
    "        tokens_frase = sentence\n",
    "        bigrammi = list(bigrams(sentence))\n",
    "        trigrammi = list(trigrams(sentence))\n",
    "        primo_token = bigrammi[0][0]\n",
    "        prob_primoToken = freq_token[primo_token] / len(tokens)\n",
    "        prob_primoBigramma = bigrammi_corpus.count(bigrammi[0])/freq_token[primo_token]\n",
    "        probMarkov1 = prob_primoToken * prob_primoBigramma\n",
    "        prob_trigrammi = 1\n",
    "        for trigramma in trigrammi:\n",
    "            prob_trigrammi *= trigrammi_corpus.count(trigramma)/bigrammi_corpus.count((trigramma[0], trigramma[1]))\n",
    "            probMarkov2 = probMarkov1 * prob_trigrammi\n",
    "            if probMarkov2 > prob_max:\n",
    "                prob_max = probMarkov2\n",
    "                frase_max = sentence\n",
    "    return \" \".join(frase_max), prob_max\n",
    "\n",
    "def distribuzione_ner(tokens, tokens_tag):\n",
    "    #utilizzando il metodo ne_chunk, effettuo una classificazione delle entità nominate presenti nel testo\n",
    "    ner_tags = nltk.ne_chunk(tokens_tag)  \n",
    "    ner_freq = Counter()\n",
    "    #conto le sole entità nominate\n",
    "    for nodo in ner_tags:\n",
    "        if hasattr(nodo, 'label'):\n",
    "            classe = nodo.label()\n",
    "            entità = \" \".join([token for token, pos in nodo.leaves()])\n",
    "            ner_freq[(entità, classe)] += 1\n",
    "    #creazione dei dizionari per organizzazioni, persone e GPE\n",
    "    organizzazioni = {}\n",
    "    persone = {}\n",
    "    gpe = {}\n",
    "    for parola, classe in ner_freq.keys():\n",
    "        if classe == 'ORGANIZATION':\n",
    "            organizzazioni[parola, classe] = ner_freq[parola, classe]\n",
    "        elif classe == 'PERSON':\n",
    "            persone[parola, classe] = ner_freq[parola, classe]\n",
    "        elif classe == 'GPE':\n",
    "            gpe[parola, classe] = ner_freq[parola, classe]\n",
    "    #seleziono le prime 15 entità più frequenti per ogni dizionario\n",
    "    top15_organizzazioni = sorted(organizzazioni.items(), key = lambda x: x[1], reverse = True)[:15]\n",
    "    top15_persone = sorted(persone.items(), key = lambda x: x[1], reverse = True)[:15]\n",
    "    top15_gpe = sorted(gpe.items(), key = lambda x: x[1], reverse = True)[:15]\n",
    "    return top15_organizzazioni, top15_persone, top15_gpe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ab418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I dieci unigrammi sono: [(('IN',), 2560), (('NN',), 2506), (('DT',), 1895), (('JJ',), 1465), ((',',), 1177), (('PRP',), 945), (('RB',), 925), (('NNS',), 923), (('VBZ',), 827), (('VB',), 748)]\n",
      "I dieci bigrammi più frequenti sono: [(('DT', 'NN'), 1009), (('IN', 'DT'), 871), (('NN', 'IN'), 801), (('JJ', 'NN'), 526), (('DT', 'JJ'), 430), (('NN', ','), 427), (('IN', 'NN'), 391), (('PRP', 'VBP'), 325), (('TO', 'VB'), 309), (('JJ', 'NNS'), 308)]\n",
      "I dieci trigrammi più frequenti sono [(('IN', 'DT', 'NN'), 480), (('DT', 'NN', 'IN'), 438), (('DT', 'JJ', 'NN'), 312), (('NN', 'IN', 'DT'), 245), (('NN', 'IN', 'NN'), 195), (('IN', 'DT', 'JJ'), 192), (('JJ', 'NN', 'IN'), 164), (('PRP', 'MD', 'VB'), 136), (('DT', 'NN', ','), 117), (('JJ', 'NN', ','), 116)]\n",
      "\n",
      "I 20 nomi più presenti nel testo analisi_mente.txt sono: [('desire', 52), ('matter', 47), ('mind', 42), ('behaviour', 39), ('consciousness', 37), ('view', 37), ('animal', 37), ('thought', 36), ('object', 33), ('something', 27), ('observation', 27), ('world', 24), ('experience', 24), ('knowledge', 24), ('content', 24), ('instinct', 22), ('question', 21), ('case', 21), ('theory', 20), ('psychology', 20)]\n",
      "I 20 aggettivi più presenti nel testo analisi_mente.txt sono: [('other', 52), ('different', 35), ('mental', 30), ('such', 30), ('conscious', 30), ('first', 26), ('certain', 23), ('own', 23), ('human', 23), ('unconscious', 23), ('same', 21), ('physical', 20), ('present', 19), ('mechanical', 15), ('instinctive', 14), ('many', 12), ('ordinary', 11), ('various', 11), ('external', 11), ('similar', 10)]\n",
      "I 20 verbi più presenti nel testo analisi_mente.txt sono: [('were', 15), ('said', 11), ('was', 10), ('had', 10), ('knew', 3), ('used', 3), ('held', 2), ('did', 2), ('spoke', 2), ('left', 2), ('desired', 2), ('put', 2), ('conceived', 1), ('called', 1), ('believed', 1), ('went', 1), ('touched', 1), ('came', 1), ('supposed', 1), ('led', 1)]\n",
      "\n",
      "I venti bigrammi aggettivo-sostantivo più frequenti nel file analisi_mente.txt: [(('other', 'hand'), 11), (('external', 'observation'), 11), (('dead', 'matter'), 8), (('physical', 'world'), 5), (('first', 'time'), 5), (('later', 'lecture'), 4), (('hungry', 'animal'), 4), (('outside', 'observer'), 4), (('great', 'deal'), 3), (('past', 'event'), 3), (('nervous', 'system'), 3), (('unstable', 'equilibrium'), 3), (('certain', 'result'), 3), (('first', 'lecture'), 2), (('psychical', 'phenomena'), 2), (('mere', 'property'), 2), (('conventional', 'psychology'), 2), (('easy', 'step'), 2), (('animal', 'psychology'), 2), (('mental', 'world'), 2)]\n",
      "\n",
      "Venti bigrammi aggettivo-sostantivo con la probabilità condizionata massima nel file analisi_mente.txt: [(('German', 'edition'), 1.0), (('unstable', 'equilibrium'), 1.0), (('stored', 'energy'), 1.0), (('prior', 'experience'), 1.0), (('restricted', 'group'), 1.0), (('middle-aged', 'man'), 1.0), (('external', 'observation'), 0.9166666666666666), (('dead', 'matter'), 0.8), (('nervous', 'system'), 0.75), (('conventional', 'psychology'), 0.6666666666666666), (('single', 'thought'), 0.6666666666666666), (('direct', 'contact'), 0.6666666666666666), (('separate', 'source'), 0.6666666666666666), (('successful', 'movement'), 0.6666666666666666), (('later', 'lecture'), 0.5714285714285714), (('peculiar', 'something'), 0.5), (('psychical', 'phenomena'), 0.5), (('comparative', 'psychology'), 0.5), (('recent', 'work'), 0.5), (('donkey', 'bray'), 0.5)]\n",
      "\n",
      "Venti bigrammi aggettivo-sostantivo con forza associata massima nel file analisi_mente.txt: [(('impassable', 'gulf'), 42.81592080070167), (('coloured', 'surface'), 42.81592080070167), (('immanent', 'objectivity'), 42.81592080070167), (('analytic', 'scrutiny'), 42.81592080070167), (('prospective', 'labours'), 42.81592080070167), (('golden', 'mountain'), 42.81592080070167), (('transcendental', 'ego'), 42.81592080070167), (('Radical', 'Empiricism'), 42.81592080070167), (('pragmatic', 'equivalent'), 42.81592080070167), (('contorted', 'mode'), 42.81592080070167), (('methodological', 'precept'), 42.81592080070167), (('undue', 'haste'), 42.81592080070167), (('unimportant', 'outcome'), 42.81592080070167), (('relative', 'justification'), 42.81592080070167), (('legs', 'respond'), 42.81592080070167), (('copious', 'flow'), 42.81592080070167), (('seasonal', 'fluctuation'), 42.81592080070167), (('extraordinary', 'accuracy'), 42.81592080070167), (('new-born', 'baby'), 42.81592080070167), (('unreflecting', 'opinion'), 42.81592080070167)]\n",
      "\n",
      "La frase con la media della distribuzione maggiore per il file analisi_mente.txt è: Take , for example , reproduction and the rearing of the young .\n",
      "\n",
      "La frase con la media della distribuzione minore per il file analisi_mente.txt è: William James 's view was first set forth in an essay called `` Does 'consciousness ' exist ?\n",
      "\n",
      "\n",
      "La frase con la probabilità più alta secondo un modello di markov di ordine 2 nel file analisi_mente.txt è: This is a different form of consciousness from any of the earlier ones . (0.00030329070413991814)\n",
      "\n",
      "I 15 nomi di organizzazioni più frequenti in analisi_mente.txt sono: [(('NOT', 'ORGANIZATION'), 4), (('City', 'ORGANIZATION'), 3), (('Psychology', 'ORGANIZATION'), 3), (('CONSCIOUSNESS', 'ORGANIZATION'), 2), (('THE', 'ORGANIZATION'), 2), (('INTO', 'ORGANIZATION'), 2), (('Experience', 'ORGANIZATION'), 2), (('Methuen', 'ORGANIZATION'), 2), (('LECTURE', 'ORGANIZATION'), 2), (('Macmillan', 'ORGANIZATION'), 2), (('Lomechusa', 'ORGANIZATION'), 2), (('ANALYSIS OF', 'ORGANIZATION'), 1), (('MIND', 'ORGANIZATION'), 1), (('RECENT', 'ORGANIZATION'), 1), (('External', 'ORGANIZATION'), 1)]\n",
      "\n",
      "I 15 nomi di persona più frequenti in analisi_mente.txt sono: [(('James', 'PERSON'), 9), (('Paul', 'PERSON'), 6), (('Freud', 'PERSON'), 5), (('Brentano', 'PERSON'), 4), (('Bergson', 'PERSON'), 4), (('Fabre', 'PERSON'), 4), (('William James', 'PERSON'), 3), (('Smith', 'PERSON'), 2), (('St. Paul', 'PERSON'), 2), (('Mr. Jones', 'PERSON'), 2), (('Idealism', 'PERSON'), 2), (('Psychology', 'PERSON'), 2), (('John B. Watson', 'PERSON'), 2), (('Boyle', 'PERSON'), 2), (('Lloyd Morgan', 'PERSON'), 2)]\n",
      "\n",
      "I 15 nomi di entità geopolitiche più presenti in analisi_mente.txt sono [(('Meinong', 'GPE'), 6), (('St.', 'GPE'), 4), (('American', 'GPE'), 3), (('Lecture', 'GPE'), 2), (('Austrian', 'GPE'), 2), (('Brentano', 'GPE'), 2), (('Brown', 'GPE'), 2), (('German', 'GPE'), 2), (('Insanity', 'GPE'), 2), (('Freudian', 'GPE'), 2), (('Indirect', 'GPE'), 1), (('Unwin', 'GPE'), 1), (('Locke', 'GPE'), 1), (('Hume', 'GPE'), 1), (('Middle Ages', 'GPE'), 1)]\n"
     ]
    }
   ],
   "source": [
    "file1 = \"analisi_mente.txt\"\n",
    "\n",
    "def main(corpus1):\n",
    "    lettura_file1 = lettura(corpus1)\n",
    "    frasi1, tokens1, tokens_tag1 = analisiCorpus(lettura_file1)\n",
    "    \n",
    "     #ORDINO PER FREQUENZA DECRESCENTE I 10 UNIGRAMMI, BIGRAMMI E TRIGRAMMI DI POS PIU' FREQUENTI\n",
    "\n",
    "    indici = [1, 2, 3]\n",
    "    unigrammi1 = pos_ngrammi(tokens_tag1, indici[0])\n",
    "    print(f'I dieci unigrammi sono: {unigrammi1}')\n",
    "    bigrammi1 = pos_ngrammi(tokens_tag1, indici[1])\n",
    "    print(f'I dieci bigrammi più frequenti sono: {bigrammi1}')\n",
    "    trigrammi1 = pos_ngrammi(tokens_tag1, indici[2])\n",
    "    print(f'I dieci trigrammi più frequenti sono {trigrammi1}')\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    #ORDINO PER FREQUENZA DECRESCENTI I 10 NOMI, AGGETTIVI E VERBI PIU' FREQUENTI\n",
    "    indici_tag = ['NN', 'JJ', 'VBD']\n",
    "    top_nomi = top_tags(tokens_tag1, indici_tag[0])\n",
    "    print(f'I 20 nomi più presenti nel testo {file1} sono: {top_nomi}')\n",
    "    top_aggettivi = top_tags(tokens_tag1, indici_tag[1])\n",
    "    print(f'I 20 aggettivi più presenti nel testo {file1} sono: {top_aggettivi}')\n",
    "    top_verbi = top_tags(tokens_tag1, indici_tag[2])\n",
    "    print(f'I 20 verbi più presenti nel testo {file1} sono: {top_verbi}')\n",
    "    \n",
    "    #ESTRAGGO I 20 BIGRAMMI COMPOSTI DA AGGETTIVO-SOSTANTIVO PIU' FREQUENTI\n",
    "    \n",
    "    print()\n",
    "    top_bigrammi1, bigrammi1 = bigrammi_nomi_aggettivi(tokens_tag1)\n",
    "    print(f'I venti bigrammi aggettivo-sostantivo più frequenti nel file {file1}: {top_bigrammi1}')\n",
    "    \n",
    "    #ESTRAGGO I 20 BIGRAMMI AGGETTIVO-SOSTANTIVO CON PROBABILITA' CONDIZIONATA MASSIMA\n",
    "    \n",
    "    prob_cond_bigrams1 = prob_cond(tokens1, bigrammi1)\n",
    "    print()\n",
    "    print(f'Venti bigrammi aggettivo-sostantivo con la probabilità condizionata massima nel file {file1}: {prob_cond_bigrams1}')\n",
    "    \n",
    "    #ESTRAGGO I 20 BIGRAMMI AGGETTIVO-SOSTANTIVO CON PMI MASSIMA\n",
    "    top_pmi1 = mutual_information(tokens1, bigrammi1)\n",
    "    print()\n",
    "    print(f'Venti bigrammi aggettivo-sostantivo con forza associata massima nel file {file1}: {top_pmi1}')\n",
    "    \n",
    "    # DATA UNA SERIE DI CARATTERISTICHE, ESTRAGGO PER IL MIO CORPUS LA FRASE CON LA MEDIA DELLA DISTRIBUZIONE DI FREQUENZA MAGGIORE \n",
    "    # E FREQUENZA MINORE PER I TOKEN, OLTRE A QUELLA CON LA PROBABILITA' PIU' ALTA CON UN MODELLO DI MARKOV DI ORDINE 2\n",
    "    \n",
    "    frase_max1 = distribuzione_frasi_max(tokens1, frasi1)\n",
    "    print()\n",
    "    print(f'La frase con la media della distribuzione maggiore per il file {file1} è: {frase_max1}')\n",
    "    print()\n",
    "    frase_min1 = distribuzione_frasi_min(tokens1, frasi1)\n",
    "    print(f'La frase con la media della distribuzione minore per il file {file1} è: {frase_min1}')\n",
    "    print()\n",
    "    print()\n",
    "    prob_markov1, prob1 = markov2(tokens1, frasi1)\n",
    "    print(f'La frase con la probabilità più alta secondo un modello di markov di ordine 2 nel file {file1} è: {prob_markov1} ({prob1})')\n",
    "    \n",
    "    #TOP 15 ENTITA' PIU' PRESENTI PER OGNI CLASSE \n",
    "    \n",
    "    top15_organizzazioni1, top15_persone1, top15_gpe1 = distribuzione_ner(tokens1, tokens_tag1)\n",
    "    print()\n",
    "    print(f'I 15 nomi di organizzazioni più frequenti in {file1} sono: {top15_organizzazioni1}\\n\\nI 15 nomi di persona più frequenti in {file1} sono: {top15_persone1}\\n\\nI 15 nomi di entità geopolitiche più presenti in {file1} sono {top15_gpe1}')\n",
    "    \n",
    "main(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b470ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
